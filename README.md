# Jerry's LLM from Scratch

## Intro
This is a repository that contains my implementation of the LLaMA-2 with reference to Datawhale and Andrej Karpathy's courses. The structure of this repo is shown as follows:

```
    Jerry-LLM-from-scratch/
    â”œâ”€â”€ REAADME.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ BelleGroup/                # Notice that this is the data for Tokenizer training & Pretraining Process, can be found on ModelScope
    â”‚   â”œâ”€â”€ README.md              # Not Included in this repo
    â”‚   â””â”€â”€ train_3.5M_CN.json     # Not Included in this repo
    â”œâ”€â”€ data/                      # Not Included in this repo
    â”‚   â”œâ”€â”€ BelleGroup_sft.jsonl   # Not Included in this repo
    â”‚   â”œâ”€â”€ mobvoi_seq_monkey_general_open_corpus.jsonl     # Not Included in this repo
    â”‚   â””â”€â”€ seq_monkey_split.jsonl   # Not Included in this repo
    â”‚   
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ Attention.py
    â”‚   â”œâ”€â”€ Decoder_Layer.py
    â”‚   â”œâ”€â”€ k_model.py
    â”‚   â”œâ”€â”€ MLP.py
    â”‚   â”œâ”€â”€ ModelConfig.py
    â”‚   â”œâ”€â”€ RMSNorm.py
    â”‚   â”œâ”€â”€ tokenizer.py
    â”‚   â””â”€â”€ Transformer.py
    â”‚   
    â”œâ”€â”€ output_models/
    â”‚   â”œâ”€â”€ sft_dim1024_layers18_vocab_size6144.pth     # Not Included in this repo
    â”‚   â””â”€â”€ ckpt/   # Not Included in this repo
    â”‚       â”œâ”€â”€ pretrain_1024_18_6144_step300000.pth
    â”‚       â””â”€â”€ pretrain_1024_18_6144.pth
    â”œâ”€â”€ seq-monkey/   # Not Included in this repo
    â”‚   â””â”€â”€ mobvoi_seq_monkey_general_open_corpus.jsonl.tar.bz2
    â”œâ”€â”€ srcs/
    â”‚   â”œâ”€â”€ inference.py
    â”‚   â”œâ”€â”€ pretrain.py
    â”‚   â””â”€â”€ SFT.py
    â”œâ”€â”€ swanlog/
    â”œâ”€â”€ Tokenizer/  # Tokenizer that is out-of-the-box
    â”‚   â”œâ”€â”€ special_tokens_map.json
    â”‚   â”œâ”€â”€ tokenizer_config.json
    â”‚   â””â”€â”€ tokenizer.json
    â””â”€â”€ utlis/
        â”œâ”€â”€ Args.py
        â”œâ”€â”€ data_manipulation.py
        â”œâ”€â”€ dataset.py
        â”œâ”€â”€ pretrain_args.py
        â””â”€â”€ split_jsonl.py
        
```

## Notice
Some of the data such as seq_monkey data and pth file are not included since there are too large to contain.

## Training Details

- Tokenizer: Around one day...

- Pretraining: Might be a "little bit" of too diffcult for my machine to do that.

- SFT: It has 2261913 iters while the whole training process takes about 3700min on a single Nvidia RTX 2080Ti (22GB VRAM) GPU. The log is shown as follows:

    ![](./imgs/SFT_log.png)

## Demonstration
The test result is shown as follows:

```
pretrain_prompt_datas = [
        '<|im_start|>äººå·¥æ™ºèƒ½æ˜¯',
        '<|im_start|>å¤§è¯­è¨€æ¨¡å‹æ˜¯',
        '<|im_start|>åå—ç†å·¥å¤§å­¦æ˜¯',
    ]
```

![](./imgs/test.png)

From the result we can see that the output text can be regard as natural language, but there are many mistakes since the params of the LLM is quite small.

(Unbengable: SCUT ğŸ¤£ğŸ¤£ğŸ¤£)

## TODO

- Make sure the whole structure is fully understand by yourself.
- Understand that a slight difference between the model and ckpt would make an LLM become mild/wild...
- The execution of the main Pretrain/SFT code has some problems with the path of the dir... (Since I moved them to the srcs folder. Can be temporarily solved by running `python -m srcs.SFT`)

## Acknowledgement & Reference

[1] Andrej Karpathy. (2023). llama2.c: Fullstack Llama 2 LLM solution in pure C. GitHub repository. https://github.com/karpathy/llama2.c

[2] Datawhale. (2025). Happy-LLM. GitHub repository. https://github.com/datawhalechina/happy-llm